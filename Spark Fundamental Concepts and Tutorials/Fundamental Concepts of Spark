{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fundamental Concepts of Spark","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLftWM0PlJBQPUl7OGkgic"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SCr5NGeu1n-v","colab_type":"text"},"source":["# **Topic to be covered**\n","\n","1. What is Spark?\n","2. Features of Spark.\n","3. Spark eco-system.\n","4. RDDs.\n","5. Real time Use Case.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"2xEex3Go2AAa","colab_type":"text"},"source":["## **What is Spark?**\n","\n","Spark is an open-source, scalable, massively parallel in-memory execution environment for running analytics applications. We can just think of it as an in-memory layer that sits about the multiple data stores, where data can be loaded into the memory and analyzed in parallel across the cluster. Coming into big data processing much like *Elastic Map Reduce(EMR)*, Spark works to distribute the data across the cluster and then process that data in parallel. The difference here is that unlike Map Reduce which shuffles the files around the disk, Spark works in memory, and that makes it much faster in processing the data than Map Reduce. It is also said to be the lighting fast unified analytics engine for big data and machine learning.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ahnO0cOG4P8O","colab_type":"text"},"source":["## **Features of Spark**\n","\n","- ***Speed:*** Spark is a swift processing framework, because its **100 times faster in memory and 10 times faster in the disc** on comparing it with **Hadoop**. Not only that, it also provides high data processing speed.\n","\n","- ***Powerful Caching:*** It has a simple programming layer that has a powerful caching and disk persistance capabilities.\n","\n","- ***Deploment:*** Spark can be deployed through mesos, Hadoop via Yarn or Sparks own cluster manager, since Spark itself was designed and developed for real-time data processing, so it offers real time computation and low latency because of in-memory competitions.\n","\n","- ***Polyglot:*** Spark provides high level API's in java, scala, python and R. Spark code can be written in any of these 4 languages. Not only that, it also provides a shell in scala and python.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"y0eKOlJX5_Ed","colab_type":"text"},"source":["## **Components of Spark eco-system**"]},{"cell_type":"code","metadata":{"id":"l5FSWxXE6JDQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}