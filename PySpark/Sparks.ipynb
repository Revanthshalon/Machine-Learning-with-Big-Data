{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sparks.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1dg0-HlCPpB5jFg4vPUtEk8iM7l2Ui2HL","authorship_tag":"ABX9TyNDhTmmaa1a2suvstyMZhPu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QYd97LWc-TwT","colab_type":"text"},"source":["# **Spark**"]},{"cell_type":"markdown","metadata":{"id":"Y-UPKcY_BxqG","colab_type":"text"},"source":["## **Setting Up Spark in Colab**"]},{"cell_type":"markdown","metadata":{"id":"9iusDnAsB1te","colab_type":"text"},"source":["**Checking the version of Java Installed in the system**"]},{"cell_type":"code","metadata":{"id":"jBzSS79__lWY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593822629913,"user_tz":-330,"elapsed":2966,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"38796cc4-7398-47cf-8649-fe1e730954f3"},"source":["!java -version"],"execution_count":1,"outputs":[{"output_type":"stream","text":["openjdk version \"1.8.0_252\"\n","OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n","OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hYxBE2sCB8Gh","colab_type":"text"},"source":["**Installing Java8**"]},{"cell_type":"code","metadata":{"id":"LGSk9PimALXP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822632703,"user_tz":-330,"elapsed":5744,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["!apt-get install openjdk-8-jdk-headless -qq> /dev/null"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXnY7SDNCB2W","colab_type":"text"},"source":["**Downloading Spark**"]},{"cell_type":"code","metadata":{"id":"qrdE7zQ5Ar3X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822664244,"user_tz":-330,"elapsed":37278,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["!wget -q http://apachemirror.wuchna.com/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.6-bin-hadoop2.7.tgz"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isNQ5lHzCGEX","colab_type":"text"},"source":["**Installing findspark**"]},{"cell_type":"code","metadata":{"id":"hG2xzfH0A_J9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593822668259,"user_tz":-330,"elapsed":41282,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"5b481202-78e9-41e6-fa42-96142cb34e44"},"source":["!pip install findspark"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8xMbPfN5CJIu","colab_type":"text"},"source":["**Setting path variables for Java and Spark**"]},{"cell_type":"code","metadata":{"id":"zORsGZ7yBCMM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822668262,"user_tz":-330,"elapsed":41275,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["import os \n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DareLR0nCZSx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822668268,"user_tz":-330,"elapsed":41274,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["import findspark\n","findspark.init()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6LMMpHS-W1I","colab_type":"text"},"source":["## **Basics of Spark DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"udNHQDdF-cwr","colab_type":"text"},"source":["- Spark DataFrames are basically used to store some data.\n","- It does hold the data in rows and columns.\n","- Each column represents a feature or variable and each row represent an individual data point.\n","- Spark DataFrames are able to deal with various sources of data, which means it can input data and output data from a variety of wide sources like csv,json and so.\n","- We can perform various transformations on data and collect the results to visualize, or record for some other processing.\n","- In order to begin working with Spark DataFrames, we need to create a Spark Session.\n","- Spark Session is like a unified entry point of a spark application, it provides a way to interact with various spark funcationalities."]},{"cell_type":"code","metadata":{"id":"iklY-xEmBgmF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822668270,"user_tz":-330,"elapsed":41269,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["# importing SparkSession\n","from pyspark.sql import SparkSession"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsACIGFNCWpi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822677382,"user_tz":-330,"elapsed":50375,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["# Creating a SparkSession\n","spark = SparkSession.builder.appName('MyFirstSparkSession').getOrCreate()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-efa7bEsCsYi","colab_type":"text"},"source":["- In this manner, we can create our spark session.\n","- We can use the session variable inside our scripts.\n","\n","Inorder, to work with real data, we need to first read a dataset. \n","- For this we can use the read method from spark context.\n","- We can also select the type of datafile we need to load, and for this, the read method has various options like csv,json and etc.\n","\n","We can load a csv file present in our filesystem as ,\n","\n","<code>\n","dataFrame_name = spark_session_variable.read.csv('filename')\n","</code>\n","\n","For Example lets load the data churn_data_st.csv "]},{"cell_type":"code","metadata":{"id":"OZD9zjEXEwqK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822683208,"user_tz":-330,"elapsed":56193,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["employee_df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/employee.csv')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDXaZCmaFZrp","colab_type":"text"},"source":["- Additional parameters such as header and inferSchema can be passed with the read.csv() method.\n","- The ***header*** parameter takes either a True or a False as its values and on giving it True, it would consider the first row of the dataset as its column title or header. If given false it would not consider it as header, rather would treat it as data.\n","- The ***inferSchema*** parameter also takes the value as either True or False. If True it would identify and assign the correct datatypes to the columns of the DataFrame based on dataset's column values, if not it would consider all the columsn as string datatype."]},{"cell_type":"code","metadata":{"id":"6MemX7mrC96g","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822683766,"user_tz":-330,"elapsed":56747,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["employee_df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/employee.csv',header=True,inferSchema=True)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjbuESIPGiNW","colab_type":"text"},"source":["- To see our created DataFrame contents and how the data looks like by using ***show()*** method.\n","- Example: `df.show()`"]},{"cell_type":"code","metadata":{"id":"OajLmpsNGj9k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822684120,"user_tz":-330,"elapsed":57093,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"7107e22e-40a9-4947-82a1-1729495e99f6"},"source":["employee_df.show()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       M002|         Bill| 64|Washington|   10|\n","|       A003|         Jeff| 56|Washington|   11|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JWBjGXE8G8e-","colab_type":"text"},"source":["- If we want to know the features or columns of the dataframe, we can get the list of columns by executing `df.columns`"]},{"cell_type":"code","metadata":{"id":"NjSJQzruHLXD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593822684121,"user_tz":-330,"elapsed":57084,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"9203fc5a-81dc-4342-a2ad-081fe02807ac"},"source":["employee_df.columns"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['employee_id', 'employee_name', 'age', 'location', 'hours']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"PHSodxBqHNMg","colab_type":"text"},"source":["- We can also use ***df.printSchema()*** to get the column data and its data types, where ***df*** being the DataFrame that we created."]},{"cell_type":"code","metadata":{"id":"bZgSOpylD5pf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593822684123,"user_tz":-330,"elapsed":57077,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"7171adf4-8fad-4ced-e9a8-46ec08ccef5f"},"source":["# Shows the datatype of the variables of the dataset\n","employee_df.printSchema()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["root\n"," |-- employee_id: string (nullable = true)\n"," |-- employee_name: string (nullable = true)\n"," |-- age: integer (nullable = true)\n"," |-- location: string (nullable = true)\n"," |-- hours: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2UDLiNvuEcpl","colab_type":"text"},"source":["## **Working with Rows and Columns in Spark DataFrame**"]},{"cell_type":"markdown","metadata":{"id":"uJzihy_AHyri","colab_type":"text"},"source":["- Here, we will try to familiarize ourselves with the Spark DataFrame.\n","- We will try to get the rows and columns data and perform some operations on them."]},{"cell_type":"markdown","metadata":{"id":"w2z6gAsmJfq3","colab_type":"text"},"source":["### **Working with Columns in Spark**"]},{"cell_type":"markdown","metadata":{"id":"njBUiIlLJjAL","colab_type":"text"},"source":["- Using the DataFrame to get to the columns directly.\n","  - Inoder to get to the columns via DataFrame, we can directly pass the column name inside of the DataFrame, that is `df['column_name']`, but on accessing the column like this we can get the column object itself."]},{"cell_type":"code","metadata":{"id":"pIAeh98TKNpo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593822684478,"user_tz":-330,"elapsed":57423,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"c5775342-7bf3-4fd2-d37d-0692dbd25398"},"source":["employee_df['age']"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Column<b'age'>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"4n90gStUKQUM","colab_type":"text"},"source":["- Using the select option in the DataFrame.\n","  - Instead of directly passing the column name as a list to the DataFrame, we can use the select method of the DataFrame by which we get the required column as a DataFrame.\n","  - **Syntax:** `df.select('column_name')`\n","  - We can also select multiple columns by passing the columns needed in the form of a list to the select function as its parameter.\n","  - **Syntax:** `df.select(['column1','column2'])`"]},{"cell_type":"code","metadata":{"id":"vEgTU6ytK93S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822684907,"user_tz":-330,"elapsed":57845,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"16b29e7d-db55-4454-90f8-e99e48ef5d13"},"source":["# Selecting and Displaying a single column\n","employee_df.select('age').show()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["+---+\n","|age|\n","+---+\n","| 47|\n","| 64|\n","| 56|\n","| 59|\n","+---+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2H3NiwakLB7t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822684909,"user_tz":-330,"elapsed":57838,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"8f5bdfb1-29e3-4cab-8c45-0d629912ee8b"},"source":["# Selecting and Displaying Employee_name an Age\n","employee_df.select(['employee_name','age']).show()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["+-------------+---+\n","|employee_name|age|\n","+-------------+---+\n","|       Pichai| 47|\n","|         Bill| 64|\n","|         Jeff| 56|\n","|         Cook| 59|\n","+-------------+---+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GxayzohOLpXV","colab_type":"text"},"source":["- Adding new Columns\n","  - We can add new columns using the method **withColumn()**. This function returns a new DataFrame by adding a new column or replacing the existing column if it has the same name as new column which we specify.\n","  - **Syntax:** `df.withColumn('new_column_name',column_iteself)`\n","  - Inorder to create a new column with the **withColumn()**, we pass in the first parameter as new column name and the second parameter as the column itself."]},{"cell_type":"code","metadata":{"id":"cK7mvLKoM4pN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822684909,"user_tz":-330,"elapsed":57830,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"290130ac-81f1-4b4c-a7a9-c5589fe8e84d"},"source":["employee_df.show()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       M002|         Bill| 64|Washington|   10|\n","|       A003|         Jeff| 56|Washington|   11|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ZBvI04UM8gH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822685254,"user_tz":-330,"elapsed":58168,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"73c24387-c03e-467f-d582-7e80374145f0"},"source":["df_new = employee_df.withColumn('overtime_time',employee_df['hours'])\n","df_new.show()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+-------------+\n","|employee_id|employee_name|age|  location|hours|overtime_time|\n","+-----------+-------------+---+----------+-----+-------------+\n","|       G001|       Pichai| 47|California|   14|           14|\n","|       M002|         Bill| 64|Washington|   10|           10|\n","|       A003|         Jeff| 56|Washington|   11|           11|\n","|       A004|         Cook| 59|California|   12|           12|\n","+-----------+-------------+---+----------+-----+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2jjhcJnbNIXD","colab_type":"text"},"source":["- We will be able to rename columns using the **withColumnRenamed()** method.\n","  - This can be achieved by passing old column name as the first parameter and the new name as the second parameter.\n","  - **Syntax:** `df.withColumnRename('old_column_name','new_column_name')`\n","  - It return a new DataFrame."]},{"cell_type":"code","metadata":{"id":"8HlTImw9NeFn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822685255,"user_tz":-330,"elapsed":58162,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["df_new = df_new.withColumnRenamed('hours','working_hours')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gMfhKo8N9I2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593822685650,"user_tz":-330,"elapsed":58550,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"45877988-b0a0-4efa-cc0b-ebc3777b61d8"},"source":["df_new.show()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-------------+-------------+\n","|employee_id|employee_name|age|  location|working_hours|overtime_time|\n","+-----------+-------------+---+----------+-------------+-------------+\n","|       G001|       Pichai| 47|California|           14|           14|\n","|       M002|         Bill| 64|Washington|           10|           10|\n","|       A003|         Jeff| 56|Washington|           11|           11|\n","|       A004|         Cook| 59|California|           12|           12|\n","+-----------+-------------+---+----------+-------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AL0WvvwIOCRE","colab_type":"text"},"source":["- Dropping a column\n","  - We can drop a column using the **drop()** method.\n","  - **Syntax:** df_new = df.drop('column_name')"]},{"cell_type":"code","metadata":{"id":"CnMyT9q7OrCp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593822685654,"user_tz":-330,"elapsed":58552,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":[""],"execution_count":20,"outputs":[]}]}