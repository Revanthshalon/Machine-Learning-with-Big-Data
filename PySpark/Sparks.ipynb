{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sparks.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1dg0-HlCPpB5jFg4vPUtEk8iM7l2Ui2HL","authorship_tag":"ABX9TyOQm5fONAT6bidbX1/SMCkB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QYd97LWc-TwT","colab_type":"text"},"source":["# **Spark**"]},{"cell_type":"markdown","metadata":{"id":"Y-UPKcY_BxqG","colab_type":"text"},"source":["## **Setting Up Spark in Colab**"]},{"cell_type":"markdown","metadata":{"id":"9iusDnAsB1te","colab_type":"text"},"source":["**Checking the version of Java Installed in the system**"]},{"cell_type":"code","metadata":{"id":"jBzSS79__lWY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593869121391,"user_tz":-330,"elapsed":4136,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"5f7eb835-d65c-4d8e-eaab-f60c5c48c569"},"source":["!java -version"],"execution_count":1,"outputs":[{"output_type":"stream","text":["openjdk version \"11.0.7\" 2020-04-14\n","OpenJDK Runtime Environment (build 11.0.7+10-post-Ubuntu-2ubuntu218.04)\n","OpenJDK 64-Bit Server VM (build 11.0.7+10-post-Ubuntu-2ubuntu218.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hYxBE2sCB8Gh","colab_type":"text"},"source":["**Installing Java8**"]},{"cell_type":"code","metadata":{"id":"LGSk9PimALXP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869124224,"user_tz":-330,"elapsed":6957,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["!apt-get install openjdk-8-jdk-headless -qq> /dev/null"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXnY7SDNCB2W","colab_type":"text"},"source":["**Downloading Spark**"]},{"cell_type":"code","metadata":{"id":"qrdE7zQ5Ar3X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869148945,"user_tz":-330,"elapsed":31672,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["!wget -q http://apachemirror.wuchna.com/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.6-bin-hadoop2.7.tgz"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isNQ5lHzCGEX","colab_type":"text"},"source":["**Installing findspark**"]},{"cell_type":"code","metadata":{"id":"hG2xzfH0A_J9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593869152512,"user_tz":-330,"elapsed":35233,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"dd977ecc-dc79-4e28-bac0-000e680e8302"},"source":["!pip install findspark"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8xMbPfN5CJIu","colab_type":"text"},"source":["**Setting path variables for Java and Spark**"]},{"cell_type":"code","metadata":{"id":"zORsGZ7yBCMM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869152514,"user_tz":-330,"elapsed":35227,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["import os \n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DareLR0nCZSx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869152515,"user_tz":-330,"elapsed":35223,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["import findspark\n","findspark.init()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6LMMpHS-W1I","colab_type":"text"},"source":["## **Basics of Spark DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"udNHQDdF-cwr","colab_type":"text"},"source":["- Spark DataFrames are basically used to store some data.\n","- It does hold the data in rows and columns.\n","- Each column represents a feature or variable and each row represent an individual data point.\n","- Spark DataFrames are able to deal with various sources of data, which means it can input data and output data from a variety of wide sources like csv,json and so.\n","- We can perform various transformations on data and collect the results to visualize, or record for some other processing.\n","- In order to begin working with Spark DataFrames, we need to create a Spark Session.\n","- Spark Session is like a unified entry point of a spark application, it provides a way to interact with various spark funcationalities."]},{"cell_type":"code","metadata":{"id":"iklY-xEmBgmF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869152515,"user_tz":-330,"elapsed":35219,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["# importing SparkSession\n","from pyspark.sql import SparkSession"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsACIGFNCWpi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869158110,"user_tz":-330,"elapsed":40809,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["# Creating a SparkSession\n","spark = SparkSession.builder.appName('MyFirstSparkSession').getOrCreate()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-efa7bEsCsYi","colab_type":"text"},"source":["- In this manner, we can create our spark session.\n","- We can use the session variable inside our scripts.\n","\n","Inorder, to work with real data, we need to first read a dataset. \n","- For this we can use the read method from spark context.\n","- We can also select the type of datafile we need to load, and for this, the read method has various options like csv,json and etc.\n","\n","We can load a csv file present in our filesystem as ,\n","\n","<code>\n","dataFrame_name = spark_session_variable.read.csv('filename')\n","</code>\n","\n","For Example lets load the data churn_data_st.csv "]},{"cell_type":"code","metadata":{"id":"OZD9zjEXEwqK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869163689,"user_tz":-330,"elapsed":46383,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["employee_df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/employee.csv')"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDXaZCmaFZrp","colab_type":"text"},"source":["- Additional parameters such as header and inferSchema can be passed with the read.csv() method.\n","- The ***header*** parameter takes either a True or a False as its values and on giving it True, it would consider the first row of the dataset as its column title or header. If given false it would not consider it as header, rather would treat it as data.\n","- The ***inferSchema*** parameter also takes the value as either True or False. If True it would identify and assign the correct datatypes to the columns of the DataFrame based on dataset's column values, if not it would consider all the columsn as string datatype."]},{"cell_type":"code","metadata":{"id":"6MemX7mrC96g","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869163690,"user_tz":-330,"elapsed":46380,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["employee_df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/employee.csv',header=True,inferSchema=True)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjbuESIPGiNW","colab_type":"text"},"source":["- To see our created DataFrame contents and how the data looks like by using ***show()*** method.\n","- Example: `df.show()`"]},{"cell_type":"code","metadata":{"id":"OajLmpsNGj9k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869164528,"user_tz":-330,"elapsed":47212,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"5cd4565c-8d4d-42e7-d484-415b5b32e216"},"source":["employee_df.show()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       M002|         Bill| 64|Washington|   10|\n","|       A003|         Jeff| 56|Washington|   11|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JWBjGXE8G8e-","colab_type":"text"},"source":["- If we want to know the features or columns of the dataframe, we can get the list of columns by executing `df.columns`"]},{"cell_type":"code","metadata":{"id":"NjSJQzruHLXD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593869164530,"user_tz":-330,"elapsed":47206,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"b134d5c2-550f-4d1d-b77e-12f4411e8bea"},"source":["employee_df.columns"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['employee_id', 'employee_name', 'age', 'location', 'hours']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"PHSodxBqHNMg","colab_type":"text"},"source":["- We can also use ***df.printSchema()*** to get the column data and its data types, where ***df*** being the DataFrame that we created."]},{"cell_type":"code","metadata":{"id":"bZgSOpylD5pf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593869164532,"user_tz":-330,"elapsed":47201,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"cf5c29ab-d378-4dac-998f-44c2bb1d05f0"},"source":["# Shows the datatype of the variables of the dataset\n","employee_df.printSchema()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["root\n"," |-- employee_id: string (nullable = true)\n"," |-- employee_name: string (nullable = true)\n"," |-- age: integer (nullable = true)\n"," |-- location: string (nullable = true)\n"," |-- hours: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2UDLiNvuEcpl","colab_type":"text"},"source":["## **Working with Rows and Columns in Spark DataFrame**"]},{"cell_type":"markdown","metadata":{"id":"uJzihy_AHyri","colab_type":"text"},"source":["- Here, we will try to familiarize ourselves with the Spark DataFrame.\n","- We will try to get the rows and columns data and perform some operations on them."]},{"cell_type":"markdown","metadata":{"id":"w2z6gAsmJfq3","colab_type":"text"},"source":["### **Working with Columns in Spark**"]},{"cell_type":"markdown","metadata":{"id":"njBUiIlLJjAL","colab_type":"text"},"source":["- Using the DataFrame to get to the columns directly.\n","  - Inoder to get to the columns via DataFrame, we can directly pass the column name inside of the DataFrame, that is `df['column_name']`, but on accessing the column like this we can get the column object itself."]},{"cell_type":"code","metadata":{"id":"pIAeh98TKNpo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593869164533,"user_tz":-330,"elapsed":47193,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"4fc3ff1e-8312-4153-c65d-6a08b89a4765"},"source":["employee_df['age']"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Column<b'age'>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"4n90gStUKQUM","colab_type":"text"},"source":["- Using the select option in the DataFrame.\n","  - Instead of directly passing the column name as a list to the DataFrame, we can use the select method of the DataFrame by which we get the required column as a DataFrame.\n","  - **Syntax:** `df.select('column_name')`\n","  - We can also select multiple columns by passing the columns needed in the form of a list to the select function as its parameter.\n","  - **Syntax:** `df.select(['column1','column2'])`"]},{"cell_type":"code","metadata":{"id":"vEgTU6ytK93S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869165044,"user_tz":-330,"elapsed":47697,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"d6334fd6-2331-435d-ca8c-ac424092ea67"},"source":["# Selecting and Displaying a single column\n","employee_df.select('age').show()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["+---+\n","|age|\n","+---+\n","| 47|\n","| 64|\n","| 56|\n","| 59|\n","+---+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2H3NiwakLB7t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869165046,"user_tz":-330,"elapsed":47691,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"1d224d6b-64a4-41f2-b23d-3818e1d37b01"},"source":["# Selecting and Displaying Employee_name an Age\n","employee_df.select(['employee_name','age']).show()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["+-------------+---+\n","|employee_name|age|\n","+-------------+---+\n","|       Pichai| 47|\n","|         Bill| 64|\n","|         Jeff| 56|\n","|         Cook| 59|\n","+-------------+---+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GxayzohOLpXV","colab_type":"text"},"source":["- Adding new Columns\n","  - We can add new columns using the method **withColumn()**. This function returns a new DataFrame by adding a new column or replacing the existing column if it has the same name as new column which we specify.\n","  - **Syntax:** `df.withColumn('new_column_name',column_iteself)`\n","  - Inorder to create a new column with the **withColumn()**, we pass in the first parameter as new column name and the second parameter as the column itself."]},{"cell_type":"code","metadata":{"id":"cK7mvLKoM4pN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869165335,"user_tz":-330,"elapsed":47972,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"e295fab1-bd2e-4613-9054-7764514abbed"},"source":["employee_df.show()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       M002|         Bill| 64|Washington|   10|\n","|       A003|         Jeff| 56|Washington|   11|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ZBvI04UM8gH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869165742,"user_tz":-330,"elapsed":48372,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"8be8455b-e425-4fb0-fad2-918bb26d096f"},"source":["df_new = employee_df.withColumn('overtime_time',employee_df['hours'])\n","df_new.show()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+-------------+\n","|employee_id|employee_name|age|  location|hours|overtime_time|\n","+-----------+-------------+---+----------+-----+-------------+\n","|       G001|       Pichai| 47|California|   14|           14|\n","|       M002|         Bill| 64|Washington|   10|           10|\n","|       A003|         Jeff| 56|Washington|   11|           11|\n","|       A004|         Cook| 59|California|   12|           12|\n","+-----------+-------------+---+----------+-----+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2jjhcJnbNIXD","colab_type":"text"},"source":["- We will be able to rename columns using the **withColumnRenamed()** method.\n","  - This can be achieved by passing old column name as the first parameter and the new name as the second parameter.\n","  - **Syntax:** `df.withColumnRename('old_column_name','new_column_name')`\n","  - It return a new DataFrame."]},{"cell_type":"code","metadata":{"id":"8HlTImw9NeFn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869165743,"user_tz":-330,"elapsed":48364,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["df_new = df_new.withColumnRenamed('hours','working_hours')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gMfhKo8N9I2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869165743,"user_tz":-330,"elapsed":48359,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"5a445dfd-df85-400d-b2cb-2ced12a4f797"},"source":["df_new.show()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-------------+-------------+\n","|employee_id|employee_name|age|  location|working_hours|overtime_time|\n","+-----------+-------------+---+----------+-------------+-------------+\n","|       G001|       Pichai| 47|California|           14|           14|\n","|       M002|         Bill| 64|Washington|           10|           10|\n","|       A003|         Jeff| 56|Washington|           11|           11|\n","|       A004|         Cook| 59|California|           12|           12|\n","+-----------+-------------+---+----------+-------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AL0WvvwIOCRE","colab_type":"text"},"source":["- Dropping a column\n","  - We can drop a column using the **drop()** method.\n","  - **Syntax:** df_new = df.drop('column_name')"]},{"cell_type":"markdown","metadata":{"id":"CnMyT9q7OrCp","colab_type":"text"},"source":["### **Working with Rows in Spark**"]},{"cell_type":"markdown","metadata":{"id":"XB9lrw0qWcsl","colab_type":"text"},"source":["- Getting the Rows and its contents.\n","  - We can get the rows data from the DataFrame with various methods. One Such function is using the **head()** function.\n","  - We can use head() function to get the top most records or rows as a list. The head() function takes on one parameter and this parameter determines how many records from the top should be selected.\n","  - **Syntax:** `df.head(number_of_records_required)`"]},{"cell_type":"code","metadata":{"id":"T6rz4nuqX5tP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593869166112,"user_tz":-330,"elapsed":48720,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"f6cfdc72-874f-4881-af3e-02d7efdb811a"},"source":["df_new.head(3)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(employee_id='G001', employee_name='Pichai', age=47, location='California', working_hours=14, overtime_time=14),\n"," Row(employee_id='M002', employee_name='Bill', age=64, location='Washington', working_hours=10, overtime_time=10),\n"," Row(employee_id='A003', employee_name='Jeff', age=56, location='Washington', working_hours=11, overtime_time=11)]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"HkcRucSTX7tn","colab_type":"text"},"source":["### **Getting to know the statistical detail of the dataset**"]},{"cell_type":"markdown","metadata":{"id":"NUF7XXyEYGco","colab_type":"text"},"source":["- We can get a statistical report or summary of all the numerical features of the dataset by using the **describe()** method.\n","- **Syntax:** `df.describe().show()`"]},{"cell_type":"code","metadata":{"id":"GdUq4lWAYUfU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1593869168144,"user_tz":-330,"elapsed":50746,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"201d2667-261f-4a8a-98d6-f1413ba1a6f3"},"source":["df_new.describe().show()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["+-------+-----------+-------------+----------------+----------+-----------------+-----------------+\n","|summary|employee_id|employee_name|             age|  location|    working_hours|    overtime_time|\n","+-------+-----------+-------------+----------------+----------+-----------------+-----------------+\n","|  count|          4|            4|               4|         4|                4|                4|\n","|   mean|       null|         null|            56.5|      null|            11.75|            11.75|\n","| stddev|       null|         null|7.14142842854285|      null|1.707825127659933|1.707825127659933|\n","|    min|       A003|         Bill|              47|California|               10|               10|\n","|    max|       M002|       Pichai|              64|Washington|               14|               14|\n","+-------+-----------+-------------+----------------+----------+-----------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_xE7iREvYWvh","colab_type":"text"},"source":["## **Using SQL in Spark DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"vWE5Wv5-ZE7-","colab_type":"text"},"source":["- Spark Dataframes gives us the option to use the SQL queries to directly work and interact with the DataFrames.\n","- In order to do this, we have to first register our DataFrame as a SQL temporary view.\n","- We can do that by using the method **createOrReplaceTempView()** and pass in the new preferred table name as parameter."]},{"cell_type":"code","metadata":{"id":"bypliCK2Z7k-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869168145,"user_tz":-330,"elapsed":50739,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["employee_df.createOrReplaceTempView('associates')"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZD3qiWBa05D","colab_type":"text"},"source":["- On executing this statement, we have successfully registered the DataFrame as SQL temporary view and now we are allowed to use the direct SQL queries on the newly registered view.\n","- We can do that by using the **sql()** method in the created spark session.\n","- **Syntax:** `variable = spark_session_variable.sql('SQL Query')`\n"]},{"cell_type":"code","metadata":{"id":"rnditlUTb5m-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869168146,"user_tz":-330,"elapsed":50736,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["sql_result = spark.sql('select * from associates')"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9eB7M1Ub_2j","colab_type":"text"},"source":["- In this way we can use complex SQL Queries to get data from the dataset. So the best thing about this method is that, if we are familiar with SQL queries we can simply use Sparks SQL features to get the required data rather than using the DataFrame operations itself."]},{"cell_type":"code","metadata":{"id":"qkWzIczVce3D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"status":"ok","timestamp":1593869168682,"user_tz":-330,"elapsed":51268,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"0f09131c-b526-4006-eafa-2d517c7b3682"},"source":["sql_result.show()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       M002|         Bill| 64|Washington|   10|\n","|       A003|         Jeff| 56|Washington|   11|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sm2LO7PZciQw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869168685,"user_tz":-330,"elapsed":51263,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["sql_result2 = spark.sql('select * from associates where age between 45 and 60 and location=\"California\"')"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArCdjJNZdOWo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593869168688,"user_tz":-330,"elapsed":51263,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"8a3ddabb-ac05-4fad-854f-bdf9c160796d"},"source":["sql_result2.show()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["+-----------+-------------+---+----------+-----+\n","|employee_id|employee_name|age|  location|hours|\n","+-----------+-------------+---+----------+-----+\n","|       G001|       Pichai| 47|California|   14|\n","|       A004|         Cook| 59|California|   12|\n","+-----------+-------------+---+----------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HFxFmeJddR-I","colab_type":"text"},"source":["## **Operations and Functions with Spark DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"pd0L9AsSdjJf","colab_type":"text"},"source":["- Once we have the data with us, we may have to perform several operations such as filter, etc."]},{"cell_type":"markdown","metadata":{"id":"sQcqCvkbd4j5","colab_type":"text"},"source":["### **Filter Method**"]},{"cell_type":"markdown","metadata":{"id":"G1eno1r_d9gh","colab_type":"text"},"source":["- This method lets us filter our some data from this dataset based on some conditions.\n","- In order to apply this method, we can simply use the **filter()** method on our DataFrame. We can use the filter() method in various ways.\n","  - **filter() method using direct SQL type Syntax.**\n","    - We can directly key in the sql like conditions within the filter() method as parameter and it does the filtering of data for us.\n","    - ***Example:*** Suppose we want to get all the items whose total amount is greater than 1000, we can directly pass this condition, in this format- `df.filter('total_amount>1000').show()`\n","  - **filter() method using DataFrame Objects.**\n","    - We can perform a similar operation as seen in the above example for filter() by using the DataFrame Objects instead of SQL like syntax. That is we can fetch the columns using the DataFrame and by using comparison operators available in python to get the same result.\n","    - ***Example:*** `df.filter(df['total_amount']>1000).show()`\n","  - **filter() method based on multiple conditions**\n","    - We can perform the filter option based on multiple conditions as well.\n","    - In order to do that we have to seperate the multiple conditons using an ampersand(&) operator, wherein each condition is enclosed within a set of parenthesis.\n","    - ***Example:*** Supposing we want to see all the items whose item price is greater than 250 and tax is lesser than 25, we can incorporate multiple conditions within filter as shown below.\n","    - `df.filter((df['item_price']>250)&(df['tax']<25)).show()`\n","- In order to filter out a specific data point or a specific row, convert it into a dictionary and access its values, we can use the filter() method pass in some condtition where we get a single data point.\n","- We can perform the filter() method and instead of using show() we can use **collect()** method. The collect() method is used to store the result into a variable. We can later use this variable to select specific values and use them in our logic or scripts.\n","  - ***Example:*** `resulting_row = df.filter(df['total_amount']).collect()`\n","  - Now, as we have the data point in form of a variable or list called `resulting_row`, we can fetch the data using the list features.\n","  - Suppose we want to get the date of the resulting item, we can easily get it by `resulting_data = resulting_row[0]['date']`.\n","  - We can also convert this into a dictionary and get the data as shown- `(resulting_row[0).asDict()`$\\rightarrow$`resulting_date_from_dictionary = (resulting_row[0]).asDict('date')`"]},{"cell_type":"code","metadata":{"id":"ehDoZIKDbzAf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869169209,"user_tz":-330,"elapsed":51776,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/items_bought.csv',inferSchema=True,header=True)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoT1-ldLcEfZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1593869169210,"user_tz":-330,"elapsed":51773,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"2229e536-099e-4c1e-c314-0af9a6882acb"},"source":["df.show(4)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["+----------+---------+----------+--------+----------+------------+\n","|      date|item_name|item_price|quantity|tax_amount|total_amount|\n","+----------+---------+----------+--------+----------+------------+\n","|11-10-2018|     Beer|     110.5|       2|     53.04|      163.54|\n","|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n","|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n","|05-10-2018|      Rum|     550.0|       2|     264.0|       814.0|\n","+----------+---------+----------+--------+----------+------------+\n","only showing top 4 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b08xNPavcFGl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593869169526,"user_tz":-330,"elapsed":52080,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"da474a04-0f76-4fb8-d3fb-146a500ca635"},"source":["df.filter(df['total_amount']>1500).show()"],"execution_count":30,"outputs":[{"output_type":"stream","text":["+----------+---------+----------+--------+----------+------------+\n","|      date|item_name|item_price|quantity|tax_amount|total_amount|\n","+----------+---------+----------+--------+----------+------------+\n","|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n","|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n","+----------+---------+----------+--------+----------+------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q6oYPXbWcUPc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593869169890,"user_tz":-330,"elapsed":52437,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"004dd274-4249-4eb0-faaa-1a31cc2fd2c9"},"source":["df.filter('total_amount > 1500').show()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["+----------+---------+----------+--------+----------+------------+\n","|      date|item_name|item_price|quantity|tax_amount|total_amount|\n","+----------+---------+----------+--------+----------+------------+\n","|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n","|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n","+----------+---------+----------+--------+----------+------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QYDhFHaPccAJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593869169891,"user_tz":-330,"elapsed":52431,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"3907a4ab-f857-468c-c96a-60f9e4c36513"},"source":["df.filter('item_price>1000 and tax_amount>500').show() # Filtering based on multiple conditions with SQL syntax"],"execution_count":32,"outputs":[{"output_type":"stream","text":["+----------+---------+----------+--------+----------+------------+\n","|      date|item_name|item_price|quantity|tax_amount|total_amount|\n","+----------+---------+----------+--------+----------+------------+\n","|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n","+----------+---------+----------+--------+----------+------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0Jvlrurco5o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593869170321,"user_tz":-330,"elapsed":52852,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"a45e7afe-3f20-47e5-9e89-fd09b73df39b"},"source":["df.filter((df['item_price']>1000)&(df['tax_amount']>500)).show() # Filtering based on multiple conditions without SQL syntax"],"execution_count":33,"outputs":[{"output_type":"stream","text":["+----------+---------+----------+--------+----------+------------+\n","|      date|item_name|item_price|quantity|tax_amount|total_amount|\n","+----------+---------+----------+--------+----------+------------+\n","|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n","+----------+---------+----------+--------+----------+------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E45-MWlkc1MQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869170322,"user_tz":-330,"elapsed":52846,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["result_data = df.filter('total_amount = 1924.74').collect() # Collecting record based on a condition"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Sj6kS77dAML","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1593869170322,"user_tz":-330,"elapsed":52841,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"54536064-689d-44cd-9177-91e3395c96f0"},"source":["result_data[0]['date'] # Fetching Data from collected Result"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'23-03-2020'"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Htp7PQN_dDOv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593869170323,"user_tz":-330,"elapsed":52835,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"5d21c0e3-ffe2-4a5d-9af6-15abc46e17eb"},"source":["result_data[0].asDict() # Converting the result into a dictionary."],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'date': '23-03-2020',\n"," 'item_name': 'Whisky',\n"," 'item_price': 1300.5,\n"," 'quantity': 2,\n"," 'tax_amount': 624.24,\n"," 'total_amount': 1924.74}"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"k0EKp3Xrdmab","colab_type":"text"},"source":["### **groupBy method**"]},{"cell_type":"markdown","metadata":{"id":"EY6Er8Vld7OY","colab_type":"text"},"source":["- **groupby()** allows us to group rows together based on some common value.\n","- If there is a column that is repeating, we can apply groupBy() on that column and we can get each and evert distict repeating value as a distinct group and we can apply logic or requirements based on certain groups.\n","- Suppose we have the revenue or sales data that contains columns like company name, products and sales figures, in this scenario we can groupby the company name column to get an overview of a specific company's products and sales figures.\n","- Let us use **groupBy()** to the company name column and use the sum function to get the total sale and revenue of each company. For this we can write the following code - `df.groupBy('company_name').sum().show()`\n","- There are various other methods we can use the groupBy function with and they are,\n","  - **count()** -  It returns the count of rows for each group.\n","  - **mean()** - It returns the mean of values of each group.\n","  - **max()** - It returns the max value in each group.\n","  - **min()** - It returns the min value in each group.\n","  - **sum()** - It return the sum of the values in each group.\n","  - **avg()** - It returns the average of values in each group.\n","  - **agg()** - It can be used to calculate more than one aggregate at a time, we will see more about the agg() method in the next topic and also learn how to use it with groupBy() method."]},{"cell_type":"markdown","metadata":{"id":"Ytk2b6A7hlKa","colab_type":"text"},"source":["### **Aggregate Function**"]},{"cell_type":"markdown","metadata":{"id":"3MK1exoQhocY","colab_type":"text"},"source":["- Aggregation Function is used to perform some aggregation operations like sum, average, and so on, on a set of values of columns and outputs a consolidated or single-valued result.\n","- Aggregate Function can be used to aggregate between all rows of the DataFrame as well as can be used with groupBy function to aggregate within rows of each group.\n","- One important point is that aggregate function takes in a dictionary as parameter. That is we can give the key and the value of the dictionary as the column name and the operation name(like sum, mean and so) respectively.\n","- Let us use the aggregate function on the entire DataFrame and later see how it can be fine tuned to work with groupBy function.\n","- Suppose, we want the total revenue or sales of all companies in the dataset and also we want to know the maximum revenue or sales among all companies, we can get it by using the **agg()** function on DataFrame as shown below,\n","  - `df.agg({'revenue_sales':'sum'}).show()` $\\rightarrow$ Total Revenue \n","  - `df.agg({'revenue_sales':'max'}).show()` $\\rightarrow$ Max Revenue\n","- Now let's see how we can use an **agg()** function with the **groupBy()** function.\n","- Suppose we want the total revenue or sales of each company present in our DataFrame. In order to achieve this we can first groupby the companies with the `company_name` column and then apply the aggregate function on the same.\n","- `df.groupBy('company_name').agg({'revenue_sales':'sum'}).show()`"]},{"cell_type":"markdown","metadata":{"id":"3FYliea0km6m","colab_type":"text"},"source":["### **orderBy Function**"]},{"cell_type":"markdown","metadata":{"id":"TSsrPWkWkrmT","colab_type":"text"},"source":["- **orderBy()** function allows us to order and sort data in our DataFrame. It is very useful when we need to quickly analyze and visualize our data.\n","- It can be used on columns of DataFrame and it acts as an ascending or decending order filter on the entire column(s).\n","- Suppose we want to order our `'revenue_sales'` column data in ascending order, we can easily achieve this by using the orderBy() function on the DataFrame and passing the column name to be sorted as the parameter to the function.\n","- `df.orderBy(df['revenue_sales']).show()`$\\rightarrow$ Ascending Order\n","- `df.orderBy(df['revenue_sales].desc()).show()`$\\rightarrow$ Descending Order"]},{"cell_type":"code","metadata":{"id":"VO3eCXKFl1Ru","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869170788,"user_tz":-330,"elapsed":53291,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/company_product_revenue.csv',inferSchema=True,header=True)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdcVzf8xl9uD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1593869170790,"user_tz":-330,"elapsed":53287,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"4bc62d0a-88f7-4120-d06f-df88e4152e77"},"source":["df.show(4)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["+-------------+------------+-------------+\n","| company_name|product_name|revenue_sales|\n","+-------------+------------+-------------+\n","|         Audi|          A4|          450|\n","|Mercedes Benz|     G Class|         1200|\n","|          BMW|          X1|          425|\n","|     Mahindra|     XUV 500|          850|\n","+-------------+------------+-------------+\n","only showing top 4 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LcnUNgoHl-rU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1593869173483,"user_tz":-330,"elapsed":55972,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"01b3c6d7-f60a-420f-88f9-e024b1df54c9"},"source":["print('Total Revenue Sales per Company')\n","df.groupBy('company_name').sum().show()"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Total Revenue Sales per Company\n","+-------------+------------------+\n","| company_name|sum(revenue_sales)|\n","+-------------+------------------+\n","|          Kia|              1140|\n","|         Audi|              2275|\n","|     Mahindra|              1640|\n","|          BMW|              1975|\n","|Mercedes Benz|              2570|\n","+-------------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rx-qqXz9mJea","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593869173483,"user_tz":-330,"elapsed":55965,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"947a5832-e124-454c-e564-52fc3e8767f9"},"source":["print('Total Revenue Sale for the entire Data')\n","df.agg({'revenue_sales':'sum'}).show()"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Total Revenue Sale for the entire Data\n","+------------------+\n","|sum(revenue_sales)|\n","+------------------+\n","|              9600|\n","+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FWJ4dKs5mYPS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1593869175520,"user_tz":-330,"elapsed":57995,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"3aee387b-2c73-48c3-e045-289fadc4b2a0"},"source":["print('Max revenue sales per Company')\n","df.groupBy('company_name').agg({'revenue_sales':'max'}).show()"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Max revenue sales per Company\n","+-------------+------------------+\n","| company_name|max(revenue_sales)|\n","+-------------+------------------+\n","|          Kia|               690|\n","|         Audi|               725|\n","|     Mahindra|               850|\n","|          BMW|               850|\n","|Mercedes Benz|              1200|\n","+-------------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqLSsL5DmueH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"ok","timestamp":1593869175521,"user_tz":-330,"elapsed":57989,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"b75db0ed-44b3-4804-ff3a-f07cb7d7639d"},"source":["print('Order the data based on revenue_sales in ascending order')\n","df.orderBy('revenue_sales').show()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Order the data based on revenue_sales in ascending order\n","+-------------+------------+-------------+\n","| company_name|product_name|revenue_sales|\n","+-------------+------------+-------------+\n","|          BMW|          X1|          425|\n","|         Audi|          A4|          450|\n","|          Kia|    Carnival|          450|\n","|Mercedes Benz|     C Class|          470|\n","|         Audi|          Q7|          500|\n","|         Audi|          A6|          600|\n","|          Kia|      Seltos|          690|\n","|          BMW|          X3|          700|\n","|         Audi|          Q5|          725|\n","|     Mahindra|     XUV 300|          790|\n","|          BMW|          X5|          850|\n","|     Mahindra|     XUV 500|          850|\n","|Mercedes Benz|         GLS|          900|\n","|Mercedes Benz|     G Class|         1200|\n","+-------------+------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CC6n8205m7TU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"ok","timestamp":1593869175522,"user_tz":-330,"elapsed":57981,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"2501d6c7-06c6-4f1b-95b3-238a3014f3e1"},"source":["print('Order the data based on revenue_sales in descending order')\n","df.orderBy(df['revenue_sales'].desc()).show()"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Order the data based on revenue_sales in descending order\n","+-------------+------------+-------------+\n","| company_name|product_name|revenue_sales|\n","+-------------+------------+-------------+\n","|Mercedes Benz|     G Class|         1200|\n","|Mercedes Benz|         GLS|          900|\n","|     Mahindra|     XUV 500|          850|\n","|          BMW|          X5|          850|\n","|     Mahindra|     XUV 300|          790|\n","|         Audi|          Q5|          725|\n","|          BMW|          X3|          700|\n","|          Kia|      Seltos|          690|\n","|         Audi|          A6|          600|\n","|         Audi|          Q7|          500|\n","|Mercedes Benz|     C Class|          470|\n","|         Audi|          A4|          450|\n","|          Kia|    Carnival|          450|\n","|          BMW|          X1|          425|\n","+-------------+------------+-------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z6xTl8RznLY4","colab_type":"text"},"source":["### **Using Standard Functions in our Operations.**"]},{"cell_type":"markdown","metadata":{"id":"tlHlGE1Enj0y","colab_type":"text"},"source":["- There are various standard functions which are very useful in our logic building and scripting, it will help us to easily manipulate the data.\n","- There are various standard functions and complex functions like correlation and standard deviation and so which we can easily use. Inorder to do this we have to import the standard functions which we want to use from the `spark.sql.functions` and it can be done by the command  - `from pyspark.sql.functions import function_name`\n","- ***Example:*** `from pyspark.sql.functions import stddev,avg,format_number`\n","- Suppose we want to get the average of the revenue or sales column from our previous dataset, and set the column name of the result to something we want. We can use standard average functions which we have imported and use the alias function to set the preferred column name as shown \n","- `df.select(avg('revenue_sales').alias('Average Revenue Sales')).show()`\n","- We can use the alias for the functions, we can define the header name of the column, by which it makes more sense and can be inferred easily.\n","- We can also perform standard deviations and see how to format the results based on our requirements.\n","- For this we can use the standard functions `'stddev'` to calculate the standard deviation and `'format_number'` function to format the results.\n","- `std_dev_result = df.select(stddev('revenue').alias('std'))`\n","- `std_dev_result.select(format_number('std',3)).show()`\n","- Here the `format_number()` method takes two parameters, the first parameter being column name ( in our case its the standard deviation result present in the `std_dev_result` DataFrame) and the second parameter being the number of decimal points considered."]},{"cell_type":"code","metadata":{"id":"vn851Bk2vQkq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869176064,"user_tz":-330,"elapsed":58517,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["df = spark.read.csv('/content/drive/My Drive/Repos/Git/Integrating Machine Learning with Big Data/PySpark/Dataset/company_product_revenue.csv',inferSchema=True,header=True)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"gteSgiulvgNv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1593869176066,"user_tz":-330,"elapsed":58514,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"2063e0c6-9ae3-4ec9-af76-d49f0a9d7cec"},"source":["df.show(4)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["+-------------+------------+-------------+\n","| company_name|product_name|revenue_sales|\n","+-------------+------------+-------------+\n","|         Audi|          A4|          450|\n","|Mercedes Benz|     G Class|         1200|\n","|          BMW|          X1|          425|\n","|     Mahindra|     XUV 500|          850|\n","+-------------+------------+-------------+\n","only showing top 4 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTTu0LFNvljl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869176067,"user_tz":-330,"elapsed":58507,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":["from pyspark.sql.functions import mean, stddev, format_number, avg # Importing in-built functions"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_Zcpg5vvhIs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593869176068,"user_tz":-330,"elapsed":58499,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"f41d1a43-774d-480a-dfbf-9156f9fbb2d1"},"source":["# Using mean functions\n","df.select(mean('revenue_sales').alias('Mean Revenue Sales')).show()"],"execution_count":47,"outputs":[{"output_type":"stream","text":["+------------------+\n","|Mean Revenue Sales|\n","+------------------+\n","| 685.7142857142857|\n","+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bt2N0V3kv4kc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593869176490,"user_tz":-330,"elapsed":58913,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"97d4a7c5-3f60-450d-c79a-fda866fa8eb3"},"source":["result_average = df.select(avg('revenue_sales').alias('Average Revenue Sales')) # Using avg Function\n","print(f'Average Revenue Sales:{result_average.head()[0]}') # Taking the first item in the head list"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Average Revenue Sales:685.7142857142857\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vO4TY_mIwaPa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1593869176491,"user_tz":-330,"elapsed":58907,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}},"outputId":"b65226b0-0351-415d-92cf-474f304ac461"},"source":["result_average.select(format_number('Average Revenue Sales',2).alias('Formatted Revenue Sales')).show() # Formatting to two decimal points."],"execution_count":49,"outputs":[{"output_type":"stream","text":["+-----------------------+\n","|Formatted Revenue Sales|\n","+-----------------------+\n","|                 685.71|\n","+-----------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K-vu_1MRxJ5Q","colab_type":"text"},"source":["## **Dealing with Missing Data in Spark DataFrame**"]},{"cell_type":"markdown","metadata":{"id":"FDP0dhwHxbRI","colab_type":"text"},"source":["- In practical scenarios the data we work with might have some empty values or missing values which are generally referred to as null values.\n","- The null values must be handled properly in our data inorder to obtain meaningful and accurate results.\n","- There are various ways we could handle the missing data issue.\n","- We could either drop the data point which contains the missing values or fill it in with some calculated or default values.\n","- It is recommended to provide a calculated result or default value to replace the missing values rather than dropping the data point as a part of preserving the data.\n"]},{"cell_type":"markdown","metadata":{"id":"iNGeu26izKlF","colab_type":"text"},"source":["### **Dropping the rows or data-points that contain the null values**"]},{"cell_type":"markdown","metadata":{"id":"g_lfh6GszP-b","colab_type":"text"},"source":["- We can drop the missing values from our DataFrame using the **drop()** method which is associated with the **na** property in the DataFrame.\n","- The drop() function returns a new DataFrame omitting all the rows with null values.\n","- ***Example:*** `df.na.drop().show()`\n","- This drops all the columns which have null values present in the `df` DataFrame.\n","- The **drop()** function have 3 parameters and they are\n","  - ***how:*** \n","    - This parameter has two main options `'any'` and `'all'`. By default, `how='any'` and it means to drop all rows if any of those rows have a null value.\n","    - The second option associated with `how` is `'all'` and it defines, drops all rows which have all row values as null.\n","    - ***Example:*** `df.na.drop(how='any').show()`\n","    - This returns us the DataFrame with those rows having one or more null values in their columns but drops all those rows which have all null values in their columns.\n","  - ***thresh:***\n","    - The `'thresh'` can be assigned with a natural number and based on the value it considers all the rows which have non-null values equal or more than threshold number which we have specified.\n","    - ***Example:*** `df.na.drop(thresh=2).show()`\n","    - It drops only those rows with non-null values less than the specified threshold value.\n","    - By default, the thresh values is set to None.\n","  - ***subset:***\n","    - The subset parameter can be used to specify the columns that we want to consider for checking or validating for null-values.\n","    - ***Example:*** `df.na.drop(subset=[column_name(s)]).show()`\n","    - This would drop the rows with how condition 'any' applied to only those specific columns which are mentioned in the subset parameter. It will still show null values present in the other columns."]},{"cell_type":"markdown","metadata":{"id":"qkqep3aL202x","colab_type":"text"},"source":["### **Filling the null-values with other values.**"]},{"cell_type":"markdown","metadata":{"id":"QIvY7J7n25cx","colab_type":"text"},"source":["- Instead of dropping the null values, let us see some methods to fill the null values and retain the rows or the data-points.\n","- We can replace the null or missing values in our DataFrame with another value using the **fill()** method which is associated with `na` method in the DataFrame.\n","- **Syntax:** `df.na.fill(value,subset=None)`\n","- Generally, the fill() function, replaces the null values by matching the data type of the column values and the value given as parameter to the fill() method.\n","- ***Example:*** `df.na.fill('filling_string_value').show()`\n","- If we apply fill() in the above manner, it will replace the column values which has the string data type and has null value with the value 'filling_string_value' which is given as the value parameter to the function fill().\n","- It functions similary with other value data types.\n","- We can use the subset parameter to select the columns which we want to fill.\n","- ***Example:*** `df.na.fill(value,subset=[column_name(s)]).show()`\n","- **Note:** If we enter a non-string value as our value parameter to the string type column, it will simply be ignored, that is the missing value will not be filled."]},{"cell_type":"markdown","metadata":{"id":"qEjm_VTW3JL2","colab_type":"text"},"source":["### **Filling Calculated values in place of missing or Null values.**"]},{"cell_type":"code","metadata":{"id":"Ws7_vkTNAB-q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593869176491,"user_tz":-330,"elapsed":58904,"user":{"displayName":"Shallun Rez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9d2AD0tXMfqM-orZBoybtWPXFojoKbvot04A9=s64","userId":"07939240584398821871"}}},"source":[""],"execution_count":49,"outputs":[]}]}