{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction to Spark.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN0QhR2uYmby7YVTjwt1Cyd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZQwnEMRIuCpb","colab_type":"text"},"source":["# **What is \"Big Data\"?**\n","\n","- Generally it is associated with 3 V's, with respect to data one deal's with, they are \"Volume-Variety-Velocity\".\n","- In simple terms, Volume is the amount of data.\n","- Variety is the kind of data, whether it is structured or unstructured.\n","- Velocity is the frequency or the momentum of the data, whether it is available on disk as a batch or real-time streaming data."]},{"cell_type":"markdown","metadata":{"id":"ZNjmk9SpvSPD","colab_type":"text"},"source":["# **Local v/s Distributed Systems**\n","\n","- Local system is a single isolated computer that shares the same RAM and HardDrive.\n","- Local systems are constrained to the number of cores available to that local system.\n","- A distributed system can be thought of as a master/slave concept, where one main computer is the master node, and we have data and process distributed onto slave nodes."]},{"cell_type":"markdown","metadata":{"id":"25f7POX_vbjH","colab_type":"text"},"source":["# **Hadoop - Overview**\n","\n","- Hadoop uses HDFS(Hadoop Distributed File System), that allows us to work with large sets of data.\n","- HDFS is the primary storage system used by Hadoop applications.\n","- Hadoop uses MapReduce which is a programming model suitable for processing of large data, these programs are parallel in nature, thus they are useful in performing large-scale data analysis using  multiple machines in cluster."]},{"cell_type":"markdown","metadata":{"id":"0rqGbkLSxbuO","colab_type":"text"},"source":["## **Hadoop Distributed File System - HDFS - Distributed Storage**\n","\n","- In HDFS, the master node is called the Name Node, which has its own CPU and RAM and it basically controls the process of distributing storage and calculations to these slave nodes.\n","- These Slave Nodes are also called as Data Nodes\n","- HDFS uses blocks or parts of data with size 128MB by default, and each of these blocks are replicated as the number of Data Nodes and these blocks are distributed in such a way, so as to enable fault tolerance.\n","- These smaller blocks of data enable parallelization during processing and multiple copies of a block prevent loss of data, even during failure of a node."]},{"cell_type":"markdown","metadata":{"id":"XUBBGpwDziBN","colab_type":"text"},"source":["## **MapReduce with Hadoop**\n","\n","- MapReduce is a way of splitting computational task to a distributed set of files such as HDFS and it comprises of a job tracker and multiple task trackers.\n","- Spark also incorporates the idea of trackers.\n","- In MapReduce, the job tracker sends the code to run on the task trackers.\n","- These task trackers, allocate CPU and Memory for the tasks and monitor the tasks in these worker nodes.\n"]},{"cell_type":"markdown","metadata":{"id":"NQ7bRTCl0oZt","colab_type":"text"},"source":["# **Spark**\n","\n","- Spark can use data stored in a varietly of formats like AWS S3, Cassandra, HDFS and more."]},{"cell_type":"markdown","metadata":{"id":"A67lCyR_0qaU","colab_type":"text"},"source":["## **Spark v/s MapReduce**\n","\n","- MapReduce writes most of the data to the disk after each map and reduce operation, which is very expensive.\n","- Spark keeps most of the data in its memory after each transformation and ability to spill over to disk if memory is filled."]},{"cell_type":"markdown","metadata":{"id":"Rwveimbh1IVv","colab_type":"text"},"source":["## **Spark RDDs**\n","\n","- Core of Spark is RDD - Resilient Distributed Dataset.\n","- RDD has 4 major features:\n","  - Distributed Collection of Data.\n","  - Fault Tolerant.\n","  - Parallel Operation.\n","  -Ability to use many data operations.\n","- RDDs are immutable and are lazily evaluvated, which means execution won't start until an action is triggered.\n","- This has various advantages like, it increases manageability and reducing complexities by allowing users to freely organize their spark program into smaller operations thus reducing the number of passes on the data by grouping the operations.\n","- Only necessary values are computed.\n","- There are two types of Spark operations-\n","  - Transformations\n","  - Actions\n","- Transformations are general instruction to follow.\n","- Actions perform the instruction and returns the Result.\n"]},{"cell_type":"markdown","metadata":{"id":"TH8p3plF3QFu","colab_type":"text"},"source":["## **Spark DataFrames**\n","\n","- A Dataset is a distributed collection of data and DataFrame is a dataset organized into named columns.\n","- It is Conceptually equivalent to a table in a relational database or Pandas DataFrame in python, but with richer optimizations.\n","- A Spark DataFrame is considered as a distributed collection of data, that is organized under named columns and provides operations to filter, group, process, and aggregate the available data.\n","- DataFrames are useful in facilitating,\n","  - Multiple Data Sources.\n","  - Multiple Programming Languages.\n","  - Processing Stuctured and Semi-Structured data.\n","  - Slicing and Dicing the data.\n","  "]},{"cell_type":"code","metadata":{"id":"PTGh22KD4nVp","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}